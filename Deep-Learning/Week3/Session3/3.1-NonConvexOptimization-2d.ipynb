{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b64ea9-b419-414a-bdfe-108de814ed39",
   "metadata": {},
   "source": [
    "# Non-Convex Optimization 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41bd3ac-f132-4979-b8fe-4dd30ea4be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from scipy.optimize import minimize\n",
    "from ipywidgets import Dropdown\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57022a-0ac4-4a61-8fd4-cf3a96bc38dd",
   "metadata": {},
   "source": [
    "###  Function to Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48cfc1-cd6e-401d-8b6f-0d87584c9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the one-dimensional non-convex function\n",
    "def non_convex_function(x):\n",
    "    return x**4 - 3*x**3 + 2\n",
    "\n",
    "# # Define the gradient of the one-dimensional non-convex function\n",
    "def gradient_non_convex(x):\n",
    "    return 4*x**3 - 9*x**2\n",
    "\n",
    "# Plot the one-dimensional non-convex function\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = non_convex_function(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label='Non-Convex Function')\n",
    "plt.title('One-dimensional Non-Convex Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f57ec9-3679-4e28-a97f-8d6987c47a8c",
   "metadata": {},
   "source": [
    "###  SGD Optimizer Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e6d12-3d2a-4061-9415-ed80e748af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization algorithms\n",
    "def sgd(func, gradient, x0, learning_rate=0.01, max_iter=100):\n",
    "    x = x0\n",
    "    trajectory = [x]\n",
    "    for _ in range(max_iter):\n",
    "        # Compute gradient of the objective function at current point\n",
    "        g = gradient(x)\n",
    "        # Update parameters using gradient descent\n",
    "        x = x - learning_rate * g\n",
    "        trajectory.append(x)\n",
    "    return np.array(trajectory)\n",
    "\n",
    "def adagrad(func, gradient, x0, learning_rate=0.1, epsilon=1e-8, max_iter=100):\n",
    "    cache = 0\n",
    "    x = x0\n",
    "    trajectory = [x]\n",
    "    for _ in range(max_iter):\n",
    "        # Compute gradient of the objective function at current point\n",
    "        g = gradient(x)\n",
    "        # Accumulate squared gradients\n",
    "        cache += g ** 2\n",
    "        # Update parameters using adaptive learning rates\n",
    "        x = x - learning_rate * g / (np.sqrt(cache) + epsilon)\n",
    "        trajectory.append(x)\n",
    "    return np.array(trajectory)\n",
    "\n",
    "def rmsprop(func, gradient, x0, learning_rate=0.1, beta=0.9, epsilon=1e-8, max_iter=100):\n",
    "    cache = 0\n",
    "    x = x0\n",
    "    trajectory = [x]\n",
    "    for _ in range(max_iter):\n",
    "        # Compute gradient of the objective function at current point\n",
    "        g = gradient(x)\n",
    "        # Update moving average of squared gradients\n",
    "        cache = beta * cache + (1 - beta) * (g ** 2)\n",
    "        # Update parameters using adaptive learning rates\n",
    "        x = x - learning_rate * g / (np.sqrt(cache) + epsilon)\n",
    "        trajectory.append(x)\n",
    "    return np.array(trajectory)\n",
    "\n",
    "def adam(func, gradient, x0, learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, max_iter=100):\n",
    "    m = 0\n",
    "    v = 0\n",
    "    x = x0\n",
    "    trajectory = [x]\n",
    "    for t in range(max_iter):  # Introduce the loop variable 't'\n",
    "        # Compute gradient of the objective function at current point\n",
    "        g = gradient(x)\n",
    "        # Update biased first moment estimate\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        # Update biased second raw moment estimate\n",
    "        v = beta2 * v + (1 - beta2) * (g ** 2)\n",
    "        # Correct bias in first moment estimate\n",
    "        m_hat = m / (1 - beta1 ** (t+1))  # Fix 't' index by adding 1 to it\n",
    "        # Correct bias in second moment estimate\n",
    "        v_hat = v / (1 - beta2 ** (t+1))  # Fix 't' index by adding 1 to it\n",
    "        # Update parameters using adaptive learning rates\n",
    "        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        trajectory.append(x)\n",
    "    return np.array(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1914d-b1b7-4350-89c5-fe706211ffe1",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8b42e-f518-4849-8653-cba78850a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial point for optimization\n",
    "x0 = -1.5\n",
    "\n",
    "# Perform optimization using Adagrad\n",
    "sgd_trajectory = sgd(non_convex_function, gradient_non_convex, x0)\n",
    "\n",
    "# Perform optimization using Adagrad\n",
    "adagrad_trajectory = adagrad(non_convex_function, gradient_non_convex, x0)\n",
    "\n",
    "# Perform optimization using RMSProp\n",
    "rmsprop_trajectory = rmsprop(non_convex_function, gradient_non_convex, x0)\n",
    "\n",
    "# Perform optimization using Adam\n",
    "adam_trajectory = adam(non_convex_function, gradient_non_convex, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027998d-3cef-4cc5-9113-00189b17e675",
   "metadata": {},
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26eff16-bb85-4600-b83a-556f2424a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the one-dimensional non-convex function\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = non_convex_function(x)\n",
    "\n",
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x, y, label='Non-Convex Function')\n",
    "ax.set_title('Non-Convex Optimization')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Initialize empty lines for the trajectories\n",
    "sgd_line, = ax.plot([], [], label='SGD', lw=2)\n",
    "adagrad_line, = ax.plot([], [], label='Adagrad', lw=2)\n",
    "rmsprop_line, = ax.plot([], [], label='RMSProp', lw=2)\n",
    "adam_line, = ax.plot([], [], label='Adam', lw=2)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "\n",
    "# Initialize the animation\n",
    "def init():\n",
    "    sgd_line.set_data([], [])\n",
    "    adagrad_line.set_data([], [])\n",
    "    rmsprop_line.set_data([], [])\n",
    "    adam_line.set_data([], [])\n",
    "    return sgd_line, adagrad_line, rmsprop_line, adam_line\n",
    "\n",
    "# Update function for animation\n",
    "def update(i, optimizer):\n",
    "    if optimizer == 'SGD':\n",
    "        trajectory = sgd_trajectory\n",
    "        line = sgd_line\n",
    "    elif optimizer == 'Adagrad':\n",
    "        trajectory = adagrad_trajectory\n",
    "        line = adagrad_line\n",
    "    elif optimizer == 'RMSProp':\n",
    "        trajectory = rmsprop_trajectory\n",
    "        line = rmsprop_line\n",
    "    else:\n",
    "        trajectory = adam_trajectory\n",
    "        line = adam_line\n",
    "    \n",
    "    line.set_data(trajectory[:i+1], non_convex_function(trajectory[:i+1]))\n",
    "    return line,\n",
    "\n",
    "# Create animations for each optimizer\n",
    "animations = {}\n",
    "for optimizer in ['SGD', 'Adagrad', 'RMSProp', 'Adam']:\n",
    "    ani = FuncAnimation(fig, update, frames=len(sgd_trajectory), init_func=init, fargs=(optimizer,), blit=True)\n",
    "    animations[optimizer] = ani\n",
    "\n",
    "# Create HTML content for each animation\n",
    "animation_html = {k: v.to_jshtml() for k, v in animations.items()}\n",
    "\n",
    "# Display animations\n",
    "HTML('\\n'.join(animation_html.values()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
