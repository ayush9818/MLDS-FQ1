{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade0845b-03f3-4e6a-a44e-d83d83a38dd8",
   "metadata": {},
   "source": [
    "# Contrastive Loss \n",
    "\n",
    "Contrastive loss encourages similar images to have similar representations and dissimilar images to have dissimilar representations.\n",
    "\n",
    "## 1. Pairwise Contrastive Loss:\n",
    "\n",
    "Pairwise contrastive loss is a simple form of contrastive loss that encourages similar pairs to have small distances and dissimilar pairs to have large distances. It is often used in Siamese networks.\n",
    "\n",
    "$$ L_{\\text{pairwise}}(y, y') = \\frac{1}{N} \\sum_{i} \\left( y \\cdot (y')^2 + (1 - y) \\cdot \\max(0, (m - y')^2) \\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88bf7040-6b49-4769-9e27-048f8fc3688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define margin for the contrastive loss\n",
    "margin = 1.0\n",
    "\n",
    "# Define the pairwise contrastive loss function\n",
    "def pairwise_contrastive_loss(y_true, y_pred):\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060db2b-3703-45b6-af3a-43d8e8a2cab1",
   "metadata": {},
   "source": [
    "## 2. Triplet Loss:\n",
    "\n",
    "Triplet loss extends pairwise contrastive loss by considering triplets of samples: an anchor, a positive sample (similar to the anchor), and a negative sample (dissimilar to the anchor). It encourages the positive sample to be closer to the anchor than the negative sample by a margin.\n",
    "\n",
    "$$L_{\\text{triplet}}(a,p,n)=\\frac{1}{N} \\sum_{i} \\max⁡(d(a,p)−d(a,n)+α,0)$$\n",
    "\n",
    "- $a$: anchor\n",
    "- $p$: positive\n",
    "- $n$: negative\n",
    "- $α$: margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa68813-a2dd-4eea-a5dd-4b388c8cabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class TripletLoss(Loss):\n",
    "    def __init__(self, margin=0.2):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        anchor, positive, negative = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2]\n",
    "        positive_distance = K.sum(K.square(anchor - positive), axis=-1)\n",
    "        negative_distance = K.sum(K.square(anchor - negative), axis=-1)\n",
    "        loss = K.maximum(positive_distance - negative_distance + self.margin, 0.0)\n",
    "        return K.mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759507e-cc5c-4ee4-812b-e526b37c29e3",
   "metadata": {},
   "source": [
    "## 3. Online Contrastive Loss:\n",
    "\n",
    "Online contrastive loss employs hard negative mining to select the most challenging negative samples within a mini-batch. This speeds up training by focusing on the most informative samples.\n",
    "\n",
    "$$ L_{\\text{online}}(y, y') = \\frac{1}{N} \\sum_{i} \\max(0, d_{\\text{pos}} - d_{\\text{neg}}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64f949c-649a-4295-b82b-616224d19458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define online contrastive loss function with hard negative mining\n",
    "def online_contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    neg_pred = K.min(square_pred, axis=1)\n",
    "    pos_pred = K.max(square_pred, axis=1)\n",
    "    loss = K.maximum(0.0, margin - pos_pred + neg_pred)\n",
    "    return K.mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce083ebf-d5c5-4e3c-8e1b-6115ce48983d",
   "metadata": {},
   "source": [
    "## 4. Margin-based Contrastive Loss:\n",
    "\n",
    "Margin-based contrastive loss introduces a margin hyperparameter that controls the minimum distance between the anchor and the negative sample compared to the anchor and the positive sample.\n",
    "\n",
    "$$ L_{\\text{margin-based}}(y, y') = \\frac{1}{N} \\sum_{i} \\max(0, d_{\\text{pos}} - d_{\\text{neg}} + \\alpha) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ded865-f44e-4a61-bbb2-d6cba10cec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define margin-based contrastive loss function\n",
    "def margin_based_contrastive_loss(y_true, y_pred):\n",
    "    margin = 0.2\n",
    "    square_pred = K.square(y_pred)\n",
    "    positive_distance = K.sum(square_pred[:, :1], axis=-1)\n",
    "    negative_distance = K.sum(square_pred[:, 1:], axis=-1)\n",
    "    loss = K.maximum(0.0, positive_distance - negative_distance + margin)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98951e6e-6fc6-4835-a06e-5bcd1e874c64",
   "metadata": {},
   "source": [
    "## 5. Angular Contrastive Loss:\n",
    "\n",
    "Angular contrastive loss operates in the embedding space by encouraging similar samples to have small angular distances (e.g., cosine similarity) and dissimilar samples to have large angular distances.\n",
    "\n",
    "$$ L_\\text{{angular}​}(y,y′)=1−\\text{cosine similarity}(a,p) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105d8746-89c1-4a95-95d7-1ba7cbbb9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CosineSimilarity\n",
    "\n",
    "# Define angular contrastive loss function\n",
    "def angular_contrastive_loss(y_true, y_pred):\n",
    "    cosine_sim = CosineSimilarity()\n",
    "    similarity = cosine_sim(y_pred[:, :1], y_pred[:, 1:])\n",
    "    return 1 - similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70be45f-2b5e-418c-8ad4-07e6402c41e0",
   "metadata": {},
   "source": [
    "## 6. Multi-modal Contrastive Loss:\n",
    "\n",
    "Multi-modal contrastive loss is used in multi-modal settings to learn a joint embedding space where similar items across modalities are closer together and dissimilar items are farther apart. The multi-modal contrastive loss function is similar to the margin-based contrastive loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00198f48-e53b-46ae-8216-f41ef3d1e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multi-modal contrastive loss function\n",
    "def multi_modal_contrastive_loss(y_true, y_pred):\n",
    "    margin = 0.2\n",
    "    square_pred = K.square(y_pred)\n",
    "    positive_distance = K.sum(square_pred[:, :1], axis=-1)\n",
    "    negative_distance = K.sum(square_pred[:, 1:], axis=-1)\n",
    "    loss = K.maximum(0.0, positive_distance - negative_distance + margin)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c15ca-96d4-4ed2-a332-78fe0f80048b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
